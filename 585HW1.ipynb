{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1, Intro to NLP, 2017\n",
    "\n",
    "#### This is due on September 22nd at 11PM. Please see detailed submission instructions below.  100 points total.\n",
    "\n",
    "##### How to do this problem set:\n",
    "\n",
    "- What version of Python should I use? 2.7\n",
    "\n",
    "- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out a supporting file, `hw_1.py`.\n",
    "\n",
    "- For all of the textual answers you have to fill out have placeholder text which says \"Answer in one or two sentences here.\" For each question, you need to replace \"Answer in one or two sentences here\" with your answer.\n",
    "\n",
    "- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Download As -> PDF) and upload to Gradescope (2)Turn in `hw_1.py` and `hw_1.ipynb` on Moodle.\n",
    "  \n",
    "- **Important:** Check your PDF before you turn it in to gradescope to make sure it exported correctly. If ipyhton notebook gets confused about your syntax it will sometimes terminate the PDF creation routine early. You are responsible for checking for these errors. If your whole PDF does not print, try running `$jupyter nbconvert --to pdf hw_1.ipynb` to identify and fix any syntax errors that might be causing problems.\n",
    "\n",
    "- **Important:** When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One convenient way to do this is by clicking `Cell -> Run All` in the notebook menu.\n",
    " \n",
    "- This assignment is designed so that you can run all cells in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Academic honesty \n",
    "\n",
    "- We will audit the Moodle code from a few dozen students, chosen at random. The audits will check that the code you wrote and turned on Moodle generates the answers you turn in on your Gradescope PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a potential case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of code on Moodle for plagiarism. Copying code from others is considered a serious case of cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell! It sets some things up for you.\n",
    "\n",
    "# This code makes plots appear inline in this document rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division  # this line is important to avoid unexpected behavior from division\n",
    "\n",
    "# This code imports your work from hw_1.py\n",
    "from NaiveBayes import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5, 4) # set default size of plots\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! Something is wrong. Check your code which loads the reviews\n",
      "Oh no! Something is wrong. Check your code which loads the reviews\n"
     ]
    }
   ],
   "source": [
    "# download the IMDB large movie review corpus from the class webpage to a file location on your computer\n",
    "import os\n",
    "\n",
    "PATH_TO_DATA = './E-Dickinson_dataset'  # set this variable to point to the location of the IMDB corpus on your computer\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'\n",
    "TRAIN_DIR = os.path.join(PATH_TO_DATA, \"train\")\n",
    "TEST_DIR = os.path.join(PATH_TO_DATA, \"test\")\n",
    "\n",
    "for label in [POS_LABEL, NEG_LABEL]:\n",
    "    if len(os.listdir(TRAIN_DIR + \"/\" + label)) == 12500:\n",
    "        print \"Great! You have 12500 {} reviews in {}\".format(label, TRAIN_DIR + \"/\" + label)\n",
    "    else:\n",
    "        print \"Oh no! Something is wrong. Check your code which loads the reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right away, this film was ridiculous. Not that it didn't have redeeming aspects",
      " For example, the best thing about this film was the beautiful background scenery. Anyone not living on the East Coast should know the South doesn't have beautiful mountains like those found in the West. I knew it was Utah right off the bat, but perhaps Dalton couldn't suppress his English accent, so they had to excuse it by saying this was a southern town. Subverting his accent into a Southern one was easier. Sure the film has plot twists, but its phony sense of place was something I couldn't get past. It's not like Utah doesn't have meth labs... so why the writers thought it necessary to pretend it was in the South is beyond me. <br /><br />One other thing in action pictures always puzzles me. Why do they always make the \"cocking\" sound effect when the character pulls out an automatic handgun? It seemed every other sound effect in this movie was a \"chuk-chich\" signifying a 9mm was loaded and ready to fire. Of course, the weapons already had rounds chambered so this was unnecessary. <br /><br />Lastly, the pyrotechnics were WAY over the top. But hey, this film was targeted to a certain 'market segment' I suppose... It's too bad. Each of the actors can act, but this film was lame.\n"
     ]
    }
   ],
   "source": [
    "# Actually reading the data you are working with is an important part of NLP! Let's look at one of these reviews\n",
    "\n",
    "print open(TRAIN_DIR + \"/neg/3740_2.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Intro to NLP in Python: types, tokens and Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types and tokens\n",
    "\n",
    "One major part of any NLP project is word tokenization. Word tokenization is the task of segmenting text into individual words, called tokens. In this assignment, we will use simple whitespace tokenization. You will have a chance to improve this for extra credit at the end of the assigment. Take a look at the `tokenize_doc` function in `hw_1.py`. **You should not modify tokenize_doc** but make sure you understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "both\n",
      "computer\n",
      "abstract.\n",
      "science\n",
      "is\n",
      "practical\n"
     ]
    }
   ],
   "source": [
    "# We have provided a tokenize_doc function in hw_1.py. Here is a short demo of how it works\n",
    "\n",
    "d1 = \"This SAMPLE doc has   words tHat  repeat repeat\"\n",
    "bow = tokenize_doc(d1)\n",
    "\n",
    "assert bow['this'] == 1\n",
    "assert bow['sample'] == 1\n",
    "assert bow['doc'] == 1\n",
    "assert bow['has'] == 1\n",
    "assert bow['words'] == 1\n",
    "assert bow['that'] == 1\n",
    "assert bow['repeat'] == 2\n",
    "\n",
    "bow2 = tokenize_doc(\"Computer science is both practical and abstract.\")\n",
    "for b in bow2:\n",
    "    print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (5 points)**\n",
    "\n",
    "Now we are going to count the word types and word tokens in the corpus. In the cell below, use the `word_counts` dictionary variable to store the count of each word in the corpus.\n",
    "Use the `tokenize_doc` function to break documents into tokens. \n",
    "\n",
    "`word_counts` keeps track of how many times a word type appears across the corpus. For instance, `word_counts[\"dog\"]` should store the number 723 -- the count of how many times the word `dog` appears in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "word_counts = defaultdict(float)  # you might want to use a defaultdict instead https://docs.python.org/2/library/collections.html\n",
    "                  # defaultdicts are often useful for NLP in python\n",
    "\n",
    "\n",
    "for label in [POS_LABEL, NEG_LABEL]:\n",
    "    for directory in [TRAIN_DIR, TEST_DIR]:\n",
    "        for fn in glob.glob(directory + \"/\" + label + \"/*txt\"):\n",
    "            file = open(fn)\n",
    "            doc = tokenize_doc(file.read())\n",
    "            for k in doc:\n",
    "                word_counts[k] += doc[k]\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay! there are 990.0 total instances of the word type dog in the corpus\n"
     ]
    }
   ],
   "source": [
    "# you should see 990 instances of the word type \"dog\" in the corpus. (updated 9/13)\n",
    "if word_counts[\"dog\"] == 990:\n",
    "    print \"yay! there are {} total instances of the word type dog in the corpus\".format(word_counts[\"dog\"])\n",
    "else:\n",
    "    print \"hrm. Something seems off. Double check your code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (5 points)**\n",
    "\n",
    "Fill out the functions `n_word_types` and `n_word_tokens` in `hw_1.py`. These functions return the total number of word types and tokens in the corpus. **important** The autoreload \"magic\" that you setup early in the assignment should automatically reload functions as you make changes and save. If you run into trouble you can always restart the notebook and clear any .pyc files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 12670 word types in the corpus\n",
      "there are 96080.0 word tokens in the corpus\n"
     ]
    }
   ],
   "source": [
    "print \"there are {} word types in the corpus\".format(n_word_types(word_counts))\n",
    "print \"there are {} word tokens in the corpus\".format(n_word_tokens(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 (5 points)**\n",
    "\n",
    "You should see a much higher number of tokens than types. Why is that? \n",
    "\n",
    "Because words repeat! Words, words, words, words - see? There are five \"words\" so far, and since a type defines a unique token in a corpus, whereas a token is an instance of a type, we can usually always expect far more tokens than types in a given corpus. The minimum proportion between the two will be 1:1, since a corpus can consist solely of unique words, but you can't have a type in a corpus without having at least one instance of that type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipf's Law\n",
    "\n",
    "**Question 1.4 (5 points)**\n",
    "\n",
    "In this section, you will verify a key statistical properties of text: [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "\n",
    "Zipf's Law describes the relations between the frequency rank of words and frequency value of words.  For a word $w$, its frequency is inversely proportional to its rank:\n",
    "\n",
    "$$count_w = K \\frac{1}{rank_w}$$\n",
    "or in other words\n",
    "$$\\log(count_w) = K - \\log(rank_w)$$\n",
    "\n",
    "for some constant $K$, specific to the corpus and how words are being defined.\n",
    "\n",
    "Therefore, if Zipf's Law holds, after sorting the words descending on frequency, word frequency decreases in an approximately linear fashion under a log-log scale.\n",
    "\n",
    "Please make such a log-log plot by ploting the rank versus frequency.  Use a scatter plot where the x-axis is the *log(rank)*, and y-axis is *log(frequency)*.  You should get this information from `word_counts`; for example, you can take the individual word counts and sort them.  dict methods `.items()` and/or `values()` may be useful.  (Note that it doesn't really matter whether ranks start at 1 or 0 in terms of how the plot comes out.) You can check your results by comparing your plots to ones on Wikipedia; they should look qualitatively similar.\n",
    "\n",
    "*Please remember to label the meaning of the x-axis and y-axis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'log(frequency)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUQAAAEKCAYAAABquCzaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ea4330f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "x = []\n",
    "y = []\n",
    "X_LABEL = \"log(rank)\"\n",
    "Y_LABEL = \"log(frequency)\"\n",
    "\n",
    "# implement me! you should fill the x and y arrays. Add your code here\n",
    "\n",
    "wc_data = word_counts.items()\n",
    "wc_data.sort(key=lambda (w,c): -c)\n",
    "\n",
    "x = [math.log(i) for i in range(1, len(wc_data)+1)]\n",
    "y = [math.log(i[1]) for i in wc_data]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(X_LABEL)\n",
    "plt.ylabel(Y_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5 (5 points)**\n",
    "\n",
    "You should see some discountinuities on the left and right sides of this figure.  Why are we seeing them on the left?  Why are we seeing them on the right?  On the right, what are those \"ledges\"?\n",
    "\n",
    "The ones on the left represent the handful of highest-ranked, most-frequent tokens. They don't line up nicely on a log-log plot because their frequencies are much, much larger than those of the mid-ranked tokens (the body of the line). The ones on the right represent the very large number of tokens that all have a low frequency: the last three ledges, for example, are the plots of tokens that have frequencies 3, 2, 1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the homework will walk you through coding a Naive Bayes classifier that can distinguish between postive and negative reviews (at some level of accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (5 pts) ** To start, implement the `update_model` function in `hw_1.py`. Make sure to read the function comments so you know what to update. Also review the NaiveBayes class variables in the `def __init__` method of the NaiveBayes class  to get a sense of which statistics are important to keep track of. Once you have implemented `update_model`, run the train model function using the code below. You’ll need to provide the path to the dataset you downloaded to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORTING CORPUS STATISTICS\n",
      "NUMBER OF DOCUMENTS IN POSITIVE CLASS: 600.0\n",
      "NUMBER OF DOCUMENTS IN NEGATIVE CLASS: 644.0\n",
      "NUMBER OF TOKENS IN POSITIVE CLASS: 36802.0\n",
      "NUMBER OF TOKENS IN NEGATIVE CLASS: 38364.0\n",
      "VOCABULARY SIZE: NUMBER OF UNIQUE WORDTYPES IN TRAINING CORPUS: 10547\n",
      "Oh no! Something seems off. Double check your code before continuing. Maybe a mistake in update_model?\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(PATH_TO_DATA, tokenizer=tokenize_doc)\n",
    "nb.train_model()\n",
    "\n",
    "if len(nb.vocab) == 252165:\n",
    "    print \"Great! The vocabulary size is {}\".format(252165)\n",
    "else:\n",
    "    print \"Oh no! Something seems off. Double check your code before continuing. Maybe a mistake in update_model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory analysis\n",
    "\n",
    "Let’s begin to explore the count statistics stored by the update model function. Use the provided `top_n` function to find the top 10 most common words in the positive class and top 10 most common words in the negative class. You don't have to code anything to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 WORDS FOR CLASS pos:\n",
      " i 688.0\n",
      " in 449.0\n",
      " that 399.0\n",
      " as 365.0\n",
      " it 362.0\n",
      " for 338.0\n",
      " my 308.0\n",
      " be 289.0\n",
      " but 267.0\n",
      " me 265.0\n",
      " her 255.0\n",
      " not 242.0\n",
      " with 216.0\n",
      " so 210.0\n",
      " his 204.0\n",
      " when 185.0\n",
      " he 174.0\n",
      " by 172.0\n",
      " on 171.0\n",
      " at 167.0\n",
      " was 162.0\n",
      " or 162.0\n",
      " we 154.0\n",
      " if 154.0\n",
      " no 151.0\n",
      " this 145.0\n",
      " from 139.0\n",
      " like 136.0\n",
      " upon 133.0\n",
      " then 133.0\n",
      " one 126.0\n",
      " an 122.0\n",
      " you 118.0\n",
      " are 118.0\n",
      " they 106.0\n",
      " all 104.0\n",
      " day 103.0\n",
      " just 100.0\n",
      " had 96.0\n",
      " their 95.0\n",
      " little 93.0\n",
      " how 91.0\n",
      " could 91.0\n",
      " thee 91.0\n",
      " him 89.0\n",
      " she 88.0\n",
      " our 88.0\n",
      " have 86.0\n",
      " would 86.0\n",
      " what 84.0\n",
      " who 83.0\n",
      " were 80.0\n",
      " sun 80.0\n",
      " will 79.0\n",
      " know 79.0\n",
      " away 75.0\n",
      " till 75.0\n",
      " some 70.0\n",
      " never 69.0\n",
      " do 69.0\n",
      " there 68.0\n",
      " than 65.0\n",
      " ' 64.0\n",
      " where 63.0\n",
      " too 63.0\n",
      " life 62.0\n",
      " nor 62.0\n",
      " go 62.0\n",
      " its 61.0\n",
      " see 61.0\n",
      " more 58.0\n",
      " may 58.0\n",
      " such 57.0\n",
      " us 56.0\n",
      " tell 56.0\n",
      " time 54.0\n",
      " god 53.0\n",
      " sea 52.0\n",
      " unto 52.0\n",
      " night 52.0\n",
      " can 50.0\n",
      " yet 50.0\n",
      " face 49.0\n",
      " still 49.0\n",
      " mine 49.0\n",
      " other 49.0\n",
      " did 48.0\n",
      " should 47.0\n",
      " — 47.0\n",
      " bird 46.0\n",
      " heaven 46.0\n",
      " far 46.0\n",
      " them 45.0\n",
      " old 44.0\n",
      " summer 44.0\n",
      " way 44.0\n",
      " death 44.0\n",
      " thou 43.0\n",
      " without 43.0\n",
      " shall 42.0\n",
      "\n",
      "TOP 10 WORDS FOR CLASS neg:\n",
      " i 743.0\n",
      " it 556.0\n",
      " that 469.0\n",
      " in 461.0\n",
      " as 366.0\n",
      " for 322.0\n",
      " my 322.0\n",
      " not 318.0\n",
      " but 290.0\n",
      " so 280.0\n",
      " be 265.0\n",
      " me 246.0\n",
      " was 216.0\n",
      " we 213.0\n",
      " when 206.0\n",
      " if 204.0\n",
      " with 203.0\n",
      " her 189.0\n",
      " or 180.0\n",
      " you 173.0\n",
      " then 161.0\n",
      " at 159.0\n",
      " this 159.0\n",
      " his 154.0\n",
      " by 153.0\n",
      " all 147.0\n",
      " from 147.0\n",
      " on 144.0\n",
      " no 144.0\n",
      " had 142.0\n",
      " could 137.0\n",
      " they 136.0\n",
      " one 133.0\n",
      " were 132.0\n",
      " like 131.0\n",
      " an 127.0\n",
      " he 127.0\n",
      " how 126.0\n",
      " too 120.0\n",
      " would 118.0\n",
      " have 114.0\n",
      " just 113.0\n",
      " what 110.0\n",
      " upon 108.0\n",
      " are 106.0\n",
      " our 104.0\n",
      " its 102.0\n",
      " little 101.0\n",
      " than 99.0\n",
      " there 94.0\n",
      " ' 93.0\n",
      " know 91.0\n",
      " him 90.0\n",
      " will 89.0\n",
      " away 86.0\n",
      " do 84.0\n",
      " day 82.0\n",
      " who 80.0\n",
      " can 78.0\n",
      " did 78.0\n",
      " death 76.0\n",
      " some 75.0\n",
      " their 74.0\n",
      " us 72.0\n",
      " she 72.0\n",
      " never 71.0\n",
      " more 70.0\n",
      " thee 69.0\n",
      " n't 68.0\n",
      " your 65.0\n",
      " heaven 65.0\n",
      " now 64.0\n",
      " has 62.0\n",
      " such 61.0\n",
      " them 60.0\n",
      " life 59.0\n",
      " see 59.0\n",
      " out 58.0\n",
      " tell 57.0\n",
      " till 57.0\n",
      " face 56.0\n",
      " should 56.0\n",
      " night 56.0\n",
      " — 56.0\n",
      " soul 55.0\n",
      " where 54.0\n",
      " go 54.0\n",
      " before 54.0\n",
      " yet 54.0\n",
      " without 54.0\n",
      " only 53.0\n",
      " nor 52.0\n",
      " must 51.0\n",
      " far 51.0\n",
      " other 51.0\n",
      " die 50.0\n",
      " may 50.0\n",
      " eyes 50.0\n",
      " sun 49.0\n",
      " cannot 48.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"TOP 10 WORDS FOR CLASS \" + POS_LABEL + \":\"\n",
    "for tok, count in nb.top_n(POS_LABEL, 100):\n",
    "    print '', tok, count\n",
    "print ''\n",
    "\n",
    "print \"TOP 10 WORDS FOR CLASS \" + NEG_LABEL + \":\"\n",
    "for tok, count in nb.top_n(NEG_LABEL, 100):\n",
    "    print '', tok, count\n",
    "print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (5 points)**\n",
    "\n",
    "Will the top 10 words of the positive/negative classes help discriminate between the two classes? Do you imagine that processing other English text will result in a similar phenomenon?\n",
    "\n",
    "No way, since the most frequent words in English (and, probably, all human languages) have affectively neutral syntactic functions; they constitute the logical architecture of an utterance into which affective content can be embedded. For example: \"I am going to X\" only has one element, X, that can carry affect (\"scream,\" \"laugh,\" \"faint,\" \"die,\" \"sleep,\" etc.), while the rest of the words merely generate situational context for that affect. Processing other English text will almost certainly lead to the same phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (5 pts) **\n",
    "\n",
    "The Naive Bayes model assumes that all features are conditionally independent given the class label. For our purposes, this means that the probability of seeing a particular word in a document with class label $y$ is independent of the rest of the words in that document. Implement the `p_word_given_label` function. This function calculates P (w|y) (i.e., the probability of seeing word w in a document given the label of that document is y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your `p_word_given_label` function to compute the probability of seeing the word “fantastic” given each sentiment label. Repeat the computation for the word “boring.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('fantastic'|pos): 0.000154458162793\n",
      "P('fantastic'|neg): 3.77720191813e-05\n",
      "P('boring'|pos): 6.18508616873e-05\n",
      "P('boring'|neg): 0.000287275265149\n"
     ]
    }
   ],
   "source": [
    "print \"P('fantastic'|pos):\",  nb.p_word_given_label(\"fantastic\", POS_LABEL)\n",
    "print \"P('fantastic'|neg):\",  nb.p_word_given_label(\"fantastic\", NEG_LABEL)\n",
    "print \"P('boring'|pos):\",  nb.p_word_given_label(\"boring\", POS_LABEL)\n",
    "print \"P('boring'|neg):\",  nb.p_word_given_label(\"boring\", NEG_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which word has a higher probability given the positive class, fantastic or boring? Which word has a higher probability given the negative class? Is this what you would expect?\n",
    "\n",
    "\"Fantastic\" and \"boring\" have higher probabilities given positive and negative classes, respectively. Yes, definitely what would I expect, since ironic usage of these words, which would invert their affect, will be far less frequent than their customary usage in a given corpus of sufficient size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 (5 pts)**\n",
    "\n",
    "In the next cell, compute the probability of the word \"car-thievery\" in the positive training data and negative training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('car-thievery'|pos): 3.3798285075e-07\n",
      "P('car-thievery'|neg): 0.0\n"
     ]
    }
   ],
   "source": [
    "print \"P('car-thievery'|pos):\",  nb.p_word_given_label(\"car-thievery\", POS_LABEL)\n",
    "print \"P('car-thievery'|neg):\",  nb.p_word_given_label(\"car-thievery\", NEG_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about \"P('car-thievery'|neg)\"? Why do you see this number? What would happen if we took the log of \"P('car-thievery'|neg)\"? What would happen if we multiplied \"P('car-thievery'|neg)\" by \"P('cliche'|neg)\"? Why might these operations cause problems for a Naive Bayes classifier?\n",
    "\n",
    "P('car-thievery'|neg) is 0 because the token must not occur in the negative class. If we took a log of 0, the heavens would split asunder, mass hysteria, dogs and cats sleeping together, etc. Multiplying any other P by 0 would yield 0, giving us no information about the corpus; it would render a Naive Bayes classifier useless because we assume conditional independence of features and so will eventually want to multiply probabilities together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5 (5 pts)**\n",
    "\n",
    "We can address the issues from question 2.4 with psuedocounts. A psuedocount is a fixed amount added to the count of each word stored in our model. Psuedocounts are used to help smooth calculations involving words for which there is little data. Implement\n",
    "`p_word_given_label_and_psuedocount` and then run the next cell. Hint: look at the slides from the lecture on pseudocounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('car-thievery'|neg): 3.46532045897e-07\n"
     ]
    }
   ],
   "source": [
    "print \"P('car-thievery'|neg):\",  nb.p_word_given_label_and_pseudocount(\"car-thievery\", NEG_LABEL, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6 (getting ready for question 2.10)**\n",
    "\n",
    "*Prior and Likelihood* \n",
    "\n",
    "As noted before, the Naive Bayes model assumes that all words in a document are independent of one another given the document’s label. Because of this we can write the likelihood of a document as:\n",
    "\n",
    "$P(w_{d1},\\cdots,w_{dn}|y_d) = \\prod_{i=1}^{n}P(w_{di}|y_d)$\n",
    "\n",
    "However, if a document has a lot of words, the likelihood will become extremely small and we’ll encounter numerical underflow. Underflow is a common problem when dealing with prob- abilistic models; if you are unfamiliar with it, you can get a brief overview on [Wikipedia](https:/en.wikipedia.org/wiki/Arithmetic_underflow). To deal with underflow, a common transformation is to work in log-space.\n",
    "\n",
    "$\\log[P(w_{d1},\\cdots,w_{dn}|y_d)] = \\sum_{i=1}^{n}\\log[P(w_{di}|y_d)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `log_likelihood` function (Hint: it should make calls to the p word given label and psuedocount function).\n",
    "Implement the `log_prior` function. This function takes a class label and returns the log of the fraction of the training documents that are of that label.\n",
    "\n",
    "There is nothing to print out for this question. But you will use these functions in a moment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7 (5 pts) **\n",
    "\n",
    "Naive Bayes is a model that tells us how to compute the posterior\n",
    "probability of a document being of some label (i.e.,\n",
    "$P(y_d|\\mathbf{w_d})$).  Specifically, we do so using bayes rule:\n",
    "\n",
    "  $P(y_d|\\mathbf{w_d}) = \\frac{P(y_d)P(\\mathbf{w_d}|y_d)}{P(\\mathbf{w_d})}$\n",
    "\n",
    "In the previous section you implemented functions to compute both\n",
    "the log prior ($\\log[P(y_d)]$) and the log likelihood\n",
    "($\\log[P( \\mathbf{w_d} |y_d)]$ ). Now, all your missing is the\n",
    "*normalizer*, $P(\\mathbf{w_d})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the normalizer by expanding $P(\\mathbf{w_d})$. You will have to use \"MathJax\" to write out the equations. MathJax is very similar to LaTeX. 99% of the MathJax you will need to write for this course (and others at U Mass) is included in the first answer of [this](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) tutorial. MathJax and LaTeX can be annoying first, but once you get a little practice, using these tools will feel like second nature.\n",
    "\n",
    "\n",
    "$P(\\mathbf{w_d}) = P(\\mathbf{w_d}|y_d)P(y_d) + P(\\mathbf{w_d}|\\sim{y_d})P(\\sim{y_d})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8 (5 pts)**\n",
    "\n",
    "One way to classify a document is to compute the unnormalized log posterior for both labels and take the argmax (i.e., the label that yields the higher unnormalized log posterior). The unnormalized log posterior is the sum of the log prior and the log likelihood of the document. Why don’t we need to compute the log normalizer here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using argmax and evaluating a number of log posteriors (here, just two), and every log posterior has the same denominator (the log normalizer), we can ignore the log normalizer. Doing so will preserve proportionality among the log posteriors since we're effectively multiplying all of them by the common denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9 (15 pts)**\n",
    "\n",
    "Implement the `unnormalized_log_posterior` function and the `classify` function. The `classify` function should use the unnormalized log posteriors but should not compute the normalizer. Once you implement the `classify` function, we'd like to evaluate its accuracy. `evaluate_classifier_accuracy` is implemented for you so you don't need to change that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.5687382298\n"
     ]
    }
   ],
   "source": [
    "print nb.evaluate_classifier_accuracy(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.10 (5 pts)**\n",
    "\n",
    "Try evaluating your model again with a pseudocount parameter of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.2109227872\n"
     ]
    }
   ],
   "source": [
    "print nb.evaluate_classifier_accuracy(500.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the accuracy go up or down when the pseudo count parameter is raised to 500? Why do you think this is?\n",
    "\n",
    "It goes down because as the pseudocount parameter (alpha) approaches infinity, it eclipses out the actual word counts in the corpus, which are comparatively tiny. I got it to hit 50% with an alpha of just over 5000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.11 (5 pts)**\n",
    "\n",
    "Our trained model can be queried to do exploratory data analysis. We\n",
    "saw that the top 10 most common words for each class were not very\n",
    "discriminative. Often times, a more descriminative statistic is a\n",
    "word's likelihood ratio. A word's likelihood ratio is defined as\n",
    "\n",
    "$LR(w)=\\frac{P(w|y=\\mathrm{pos})}{P(w|y=\\mathrm{neg})}$\n",
    "\n",
    "A word with $LR=5$ is five times more likely to appear in a positive\n",
    "review than it is in a negative review; a word with $LR=0.33$ is one\n",
    "third as likely to appear in a positive review than a negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIKEHOOD RATIO OF 'fantastic': 4.06091371082\n",
      "LIKEHOOD RATIO OF 'boring': 0.216217429124\n",
      "LIKEHOOD RATIO OF 'the': 1.03406562348\n",
      "LIKEHOOD RATIO OF 'to': 0.943418253584\n"
     ]
    }
   ],
   "source": [
    "# Implement the nb.likelihod_ratio function and use it to investigate the likelihood ratio of \"fantastic\" and \"boring\"\n",
    "print \"LIKEHOOD RATIO OF 'fantastic':\", nb.likelihood_ratio('fantastic', 1.0)\n",
    "print \"LIKEHOOD RATIO OF 'boring':\", nb.likelihood_ratio('boring', 1.0)\n",
    "print \"LIKEHOOD RATIO OF 'the':\", nb.likelihood_ratio('the', 1.0)\n",
    "print \"LIKEHOOD RATIO OF 'to':\", nb.likelihood_ratio('to', 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it make sense that $LR$('fantastic') $>$ $LR$('to')? \n",
    "\n",
    "Yes, because it is a word with an inherent positive affect, used primarily in contexts where a speaker is indicating great satisfaction, surprised happiness, joy, etc. in response to the subject. \"To\" is structural rather than affective, so its $LR$ should be close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 2.12 (15 pts)** \n",
    "\n",
    "Find a review that your classifier got wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FN: /home/tck/PycharmProjects/CS585HW1/large_movie_review_dataset/test/pos/11646_8.txt\n",
      "In /test/pos, but label is: neg \n",
      "\n",
      "Just to clarify, Matthew Poncelet wasn't a real person, but a character combination of 2 killers who were BOTH convicted and sentenced to die for a murder of two teenagers.<br /><br />I read the User Comments and they react as if Matthew was real. The character is based on a mixture of two killers, Elmo Patrick Sonnier and Robert Lee Willie (who murdered separate people) and the murder itself was based on the one Willie committed. The conflict of both Willie having someone else present and both parties swearing the other did the killing is worked into the story as well.<br /><br />Prejean's approach is unique in that she not only is ministering to the convicts as they wait for their death and aiding them in taking responsibility for their actions, she also reaches out to the victims' families, to help them know that the convict did, indeed feel remorse for what they did-effectively aiding both parties.<br /><br />Everyone posting here seems to have strong beliefs on the Death Sentence. It's not my place to say it's right or wrong-in theory punishing death with death makes some sort of Karmic sense, however denying a person their freedom for the rest of their days, although costly, makes more sense to me-being stuck in a small room 23/7 (with one hour of exercise)for the rest of their days to be reminded of the cruel thing they did seem a more apt punishment-they are technically alive, but denied living. Say someone killed someone so they could get out of the responsibility the person they killed required (like Susan Smith killing her poor kids by shoving her car into a lake). I find it fittingly ironic that they would not get that \"freedom\" they craved and would now have to spend the rest of their days imprisoned.<br /><br />Prejean's point comes through the story very well. She has my respect-she manages to find that balance-she isn't supporting a killer, she is guiding them to accepting what they did. If they didn't feel some kind of remorse, they wouldn't be asking for spiritual guidance.<br /><br />Ona final note, when Poncelet apologizes to Delacroix parent for killing his son, the parents of the girl who was also murdered mutters something about why he didn't apologize for her death. I think the point was that throughout the movie, Poncelet denies killing both kids. There is doubt in Prejean's mind he did both killings-there is a friend who was sentenced but not to death-my thought is that Poncelet killed the Delacroix boy and the other man murdered the girl-hence Poncelet was taking responsibility for what he did. Had he been responsible for the girl's death, he probably would have apologized for that as well.\n"
     ]
    }
   ],
   "source": [
    "# in this cell, print out a review that your classifier got wrong. Print out the text of the review along with the label\n",
    "for label in [POS_LABEL]:\n",
    "    for directory in [TEST_DIR]:\n",
    "        for fn in glob.glob(directory + \"/\" + label + \"/*txt\"):\n",
    "            file = open(fn)\n",
    "            doc = tokenize_doc(file.read())\n",
    "            res = nb.classify(doc, 1)\n",
    "            if res == NEG_LABEL:\n",
    "                print \"FN:\", fn\n",
    "                print \"In /test/pos, but label is:\", res, \"\\n\"\n",
    "                file.seek(0)\n",
    "                out = file.readlines()\n",
    "                for w in out:\n",
    "                    print w\n",
    "                break\n",
    "            file.close()\n",
    "        else:\n",
    "            continue\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are two reasons your system might have misclassified this example? What improvements could you make that may help your system classify this example correctly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1: The affect of the diction is overwhelmingly negative when considered simply as a bag of words: lots of \"kill,\" \"death,\" \"murder,\" \"punish,\" \"victim,\" etc. But the writer is clearly very engaged with the world of the film and has written about the film in such detail in order to defend the quality of its characterizations; hence, it's an implicitly positive review. At present, the classifier is just dumbly (naively?) building its pos/neg model on the frequency of word counts on labeled texts in the training set. It's incapable of detecting subtleties introduced at the compositional level. To improve this, another function might be written that checks for the presence of compositional tokens typically used to indicate approval discursively (for example, \"I find it fittingly ironic\" or \"point comes through very well\" or \"has my respect\": all of these indicate the writer's approval of how the film handled its material). The presence of these tokens could be included as weights in the $LR$. This is probably one of the simpler ways to introduce some compositional analysis, but such a function would be brittle, relying on a predetermined list of tokens selected by an expert. It would lack generalizability and add a considerable amount of computational complexity.\n",
    "\n",
    "#2. There are a considerable number of \"not\" and \"-n't\"s in this review. Out of curiosity, I generated $LR$s for both: \"not\" = 0.862887136273 and \"n't\" = 0.487664310252, so it seems \"n't\" is more strongly correlated with a negative review, but \"not\" is fairly close to parity. Perhaps excluding \"not\" from the $LR$ calculation would help; after all, \"not\" doesn't have an inherent affect, but serves a logical function, especially in informal double negatives in English: \"I'm not not happy with this film\" and \"I'm not unhappy with this film\" are both expressions of mild approval, but treating \"not\" as an inherently negative term would swing both statements toward a negative misclassification.\n",
    "\n",
    "#3. Expand the training set to include more nuanced reviews like this; that will inform the model that just because kill, kill, kill shows up in a review, it's not necessarily negative. This might have minimal effect unless a much larger body of nuanced texts were introduced into the labeled training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra credit (up to 10 points) **\n",
    "\n",
    "If you don't want to do the extra credit, you can stop here! Otherwise... keep reading... \n",
    " \n",
    "In this assignment, we use whitespace tokenization to create a bag-of-unigrams representation for the movie reviews. It is possible to improve this represetation to improve your classifier's performance. Use your own code or an external library such as nltk to perform tokenization, text normalization, word filtering, etc. Fill out your work in `def tokenize_doc_and_more` (below) and then show improvement by running the following.\n",
    "\n",
    "`nb = NaiveBayes(PATH_TO_DATA, tokenizer=tokenize_doc_and_more)\n",
    "nb.train_model()\n",
    "nb.evaluate_classifier_accuracy(1.0)\n",
    "`\n",
    "\n",
    "Roughly speaking, the larger performance improvement, the more extra credit. However, doing a good job investigating, explaining and justifying your work with small experiments and comments is also extremely important. Make sure to describe what you did and analyze why your method works. Use this ipython notebook to show your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc_and_more(doc):\n",
    "    \"\"\"\n",
    "    Return some representation of a document.\n",
    "    At a minimum, you need to perform tokenization, the rest is up to you.\n",
    "    \"\"\"\n",
    "    # Implement me!\n",
    "    \n",
    "    import nltk, re, pprint\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "#     doc = doc.replace('<', ' <')\n",
    "#     doc = doc.replace('>', '> ')\n",
    "    doc = doc.replace('n\\'t', ' n\\'t')\n",
    "#     doc = doc.replace('<br />', ' ')\n",
    "    doc = doc.replace('!', ' ! ')\n",
    "    doc = doc.replace('?', ' ? ')\n",
    "\n",
    "    bow = defaultdict(float)\n",
    "    tokens = doc.split()\n",
    "    lowered_tokens = map(lambda t: t.lower(), tokens)\n",
    "    \n",
    "    common_set = ['the', 'of', 'and', 'a', 'to',\n",
    "                  'is']\n",
    "    \n",
    "    for token in lowered_tokens:\n",
    "        bow[token] += 1.0\n",
    "    \n",
    "    # remove whatever's after \"not\"?\n",
    "    # your code goes here\n",
    "    for w in common_set:\n",
    "        if w in bow:\n",
    "            del bow[w]\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORTING CORPUS STATISTICS\n",
      "NUMBER OF DOCUMENTS IN POSITIVE CLASS: 600.0\n",
      "NUMBER OF DOCUMENTS IN NEGATIVE CLASS: 644.0\n",
      "NUMBER OF TOKENS IN POSITIVE CLASS: 30531.0\n",
      "NUMBER OF TOKENS IN NEGATIVE CLASS: 32249.0\n",
      "VOCABULARY SIZE: NUMBER OF UNIQUE WORDTYPES IN TRAINING CORPUS: 10527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58.568738229755176"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayes(PATH_TO_DATA, tokenizer=tokenize_doc_and_more)\n",
    "nb.train_model()\n",
    "nb.evaluate_classifier_accuracy(2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cells at the bottom of this notebook to explain what you did in `better_tokenize_doc`. Include any experiments or explanations that you used to decide what goes in your function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At first, I tried a noise-cleaning approach, filtering out the top identical terms in each set, most of which don't carry any inherent positive or negative affect. But after an initial boost, the accuracy started to decline as the set of top common terms increased. I abandoned this approach after about the top-60 common words, except for filtering for the top five or six words.\n",
    "\n",
    "Then I fiddled with removing the annoying HTML \"br /\" tags, since I could see from looking at some sample texts that they were joining random words together into big and often unique tokens that didn't carry any inherent information about the positivity of a review. Sadly, removing them only reduced accuracy! The grammar fiend in me was irked until I considered that the number of paragraphs a reviewer types in might have some relation to their emotional state while they write; perhaps the \"br /\" tags do carry some implicit emotional feature, then.\n",
    "\n",
    "Spurred by this thought, I tried breaking off \"n't\" as an independent token; perhaps a person writing a negative review might be more likely to use this negation? The word counts for \"n't\" in the positive and negative sets are 13,114 and 19,364 respectively, a difference of 67% and surely a good $LR$ indicator. Adding this as a separate token did improve accuracy.\n",
    "\n",
    "Then I thought to try exclamation marks and question marks as independent tokens; they carry emotional content in this kind of text. Counting them separately also helped boost accuracy.\n",
    "\n",
    "Finally, I fiddled with alpha and found that it made a small but noticeable difference, but only when increased. An alpha of 2.4 produced the best result with the other changes.\n",
    "\n",
    "For all that, I only improved accuracy by .888%. This was an interesting problem to play with -- I think I spent more time on it than on the rest of the homework combined -- but I can see that it's more complicated than I assumed at first. Perhaps improving the tokenizer will only take one so far when using a NB model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
